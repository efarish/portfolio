# Project: Fine-tune a Llama 2 Hugging Face pretrained model.

This project fine-tunes a Meta Llama 2 base model using Hugging Face APIs using LORA (lower-rank adaptation) PEFT (parameter efficient training) and Bits And Bytes for the quantization.

For details, see the two notebooks below:

- train.ipynb: Used to fine-tune the Llama 2 model.
- Inference.ipynb: Used to generate text from the model trained in train.ipynb.