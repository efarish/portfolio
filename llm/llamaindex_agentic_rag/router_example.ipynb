{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5deb1d8",
   "metadata": {},
   "source": [
    "### LlamaIndex: Router Query Engine\n",
    "\n",
    "Here, I experiment with LlamaIndex routers. \n",
    "\n",
    "Please reference this [DeepLearning.AI](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/it0jz/router-query-engine) course for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a8a4d",
   "metadata": {},
   "source": [
    "### Load Environment\n",
    "\n",
    "Load environment variables and allow asyncio to be used in the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be655885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import llama_index.core\n",
    "llama_index.core.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafc5f1",
   "metadata": {},
   "source": [
    "### Retrieve and Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10040d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "files = ['https://arxiv.org/pdf/2505.10543', 'https://arxiv.org/pdf/2505.11423']\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "for idx, f in enumerate(files):\n",
    "    r = httpx.get(f, timeout=20)\n",
    "    with open(f'./data/file{idx}.pdf', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./data/file0.pdf\"]).load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Number of nodes after splitting: {len(nodes)}\")\n",
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a9eb0",
   "metadata": {},
   "source": [
    "### Configure LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a6e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"o4-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ee522",
   "metadata": {},
   "source": [
    "### Create indices\n",
    "\n",
    "Vector and Summary indices are created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fcd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828ccbe",
   "metadata": {},
   "source": [
    "### Create Query Engines.\n",
    "\n",
    "Combines doing lookups on the indices and query the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9879ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37aed14",
   "metadata": {},
   "source": [
    "### Create The Tools \n",
    "\n",
    "Create the tools that provide metadata about the query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207c8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef89407",
   "metadata": {},
   "source": [
    "### Create A Router\n",
    "\n",
    "Below, a router query engine and its selector are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e9664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab265a",
   "metadata": {},
   "source": [
    "### Submitting Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d285124",
   "metadata": {},
   "source": [
    "#### Request A Summary Of The Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "613559eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document and its conclusions, which aligns with choice 1's focus on summarization..\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Provide a summary of the document and its conclusions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af8b9652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper investigates how well open-source large language models (LLMs) can learn, reason, and plan on the fly when faced with simple dynamic tasks,\n",
      "without any additional fine-tuning.  The authors assemble a unified “agent” framework that, at each time step, can be equipped with one or more of\n",
      "three in-context modules:  \n",
      "• Self-Reflection, which reviews the sequence of past states, actions and rewards to suggest how to improve future\n",
      "choices;  \n",
      "• Oracle (heuristic mutation), which evolves rule-like heuristics across episodes via a simple evolutionary strategy;  \n",
      "• Planner, which\n",
      "looks ahead a few steps by simulating possible action sequences and scoring their expected cumulative rewards.  \n",
      "\n",
      "They evaluate four models of\n",
      "increasing size (8 B to 70 B parameters) on four “SmartPlay” environments:  \n",
      "1. Two-armed bandit (exploration/exploitation)  \n",
      "2. Rock-Paper-Scissors\n",
      "(adapting to an opponent’s biased play)  \n",
      "3. Tower of Hanoi with three disks (spatial planning)  \n",
      "4. Messenger (navigating text descriptions, avoiding\n",
      "an enemy, delivering a message)  \n",
      "\n",
      "Key findings:  \n",
      "– Model size remains the single strongest predictor of raw performance; the largest model\n",
      "consistently beats smaller ones across all tasks.  \n",
      "– Carefully designed prompting (reflection, heuristics, planning) can sometimes let a mid-size\n",
      "model match or even surpass the baseline performance of a larger model—especially on complex tasks—but these gains are highly inconsistent.  In many\n",
      "runs the same prompt design causes a mid-sized model to do much worse than its own base prompt.  \n",
      "– On very simple reactive tasks (like the bandit),\n",
      "richer prompts actually degrade performance for smaller models: the extra context dilutes signal and leads them to “overthink” and explore too long\n",
      "instead of exploiting.  \n",
      "– Advanced prompting offers the largest upside for smaller models on tasks requiring multi-step reasoning or planning, but\n",
      "provides only marginal benefit to already strong large models.  \n",
      "– Across all models and tasks, there is little sign of genuine emergent reasoning or\n",
      "self-learning.  Agents still make basic mistakes: placing larger disks on smaller ones in Hanoi, getting stuck in loops, mis­identifying objects in\n",
      "the text-based navigation, and failing to form stable long-term plans.  \n",
      "\n",
      "The authors also show that transforming sparse rewards into a task-aligned\n",
      "dense reward signal can yield more reliable learning improvements than prompt tinkering alone.  Overall, these results argue that static benchmarks\n",
      "(e.g. question–answer pairs, math word problems) mask fundamental gaps in planning, spatial coordination, and adaptive reasoning, and they call for\n",
      "richer dynamic evaluations and for future work to combine in-context methods with external memory, symbolic reasoning layers, or grounded multimodal\n",
      "perception.\n"
     ]
    }
   ],
   "source": [
    "import textwrap \n",
    "\n",
    "wrapped_text = textwrap.fill(str(response), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a45d83",
   "metadata": {},
   "source": [
    "Lets look at some of the metadata for the results. As expected, the router selected the summary query engine to service the query and, therefore, all 20 nodes of content were used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee0d3e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question asks for a summary of the document and its conclusions, which aligns with choice 1's focus on summarization.\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(response.metadata['selector_result'].reason)\n",
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb77cd",
   "metadata": {},
   "source": [
    "#### Ask A More Specific Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4544f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question seeks specific experimental results from the paper, so retrieving context is most relevant..\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the results of the Two-armed bandit evaluation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb5fdd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the two-armed bandit experiments the key findings were:\n",
      "\n",
      "• Baseline (simple count-and-exploit) wins for smaller and mid-sized models.  \n",
      "  – LLAMA\n",
      "3-8B: Baseline median ≈ 40.35 (CI 37.45–41.65) → Reflection+Planner drops to ≈ 34.00 (30.00–35.00).  \n",
      "  – DEEPSEEK-R1-14B: Baseline ≈ 41.00\n",
      "(40.55–41.40) → Reflection+Planner ≈ 32.05 (29.00–33.00).\n",
      "\n",
      "• Only the largest model benefits from more complex prompting.  \n",
      "  – LLAMA 3.3-70B:\n",
      "Baseline max ≈ 41.90 → Reflection+Planner max ≈ 48.00.\n",
      "\n",
      "• Why complexity hurts smaller models:  \n",
      "  1. Extra prompt text dilutes the reward‐count\n",
      "signal, lowering signal-to-noise.  \n",
      "  2. Reflection/Oracle/Planner encourage continued exploration even when one arm is clearly better, causing the\n",
      "agent to “overthink” and converge more slowly.\n",
      "\n",
      "• Overall, sheer model size drives the strongest performance; in-context prompting alone cannot fully\n",
      "bridge the gap.\n"
     ]
    }
   ],
   "source": [
    "wrapped_text = textwrap.fill(str(response), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dee0e5",
   "metadata": {},
   "source": [
    "As this was a specific question, the specific context query engine was used with only 2 nodes used to generate the LLM response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question seeks specific experimental results from the paper, so retrieving context is most relevant.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(response.metadata['selector_result'].reason)\n",
    "print(len(response.source_nodes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
