{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a2756a0",
   "metadata": {},
   "source": [
    "# LLM Tool Calling\n",
    "\n",
    "Example of LLM using tooling to infer parameters to call function.\n",
    "\n",
    "Please reference this [DeepLearning.AI](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/sgmbf/tool-calling) course for more details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42570f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.36'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import llama_index.core\n",
    "llama_index.core.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061fb72",
   "metadata": {},
   "source": [
    "The Llamaindex `FunctionTool` module wraps the Python function below so the can be called by the LLM. The type annotations and doc strings are view import and they will be used in prompt for the LLM tooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993745c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def mystery(x: int, y: int) -> int: \n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f0cb4",
   "metadata": {},
   "source": [
    "Below, the LLM decides which tool to call and synthesizes a call to that tool using the query below. Like a LlamaIndex router, it picks the tool. But also decides what parameters to pass the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4d9cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"o4-mini\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery function on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f73a89",
   "metadata": {},
   "source": [
    "The following introduces Agentic meta data tags that are used to return a more precise result.\n",
    "\n",
    "The code below assumes the PDFs referenced below have been stored in the `data` directory which was done in the `router_example.ipynb` router example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca16cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./data/file0.pdf\"]).load_data()\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a22b3",
   "metadata": {},
   "source": [
    "Lets look at a node with the meta listed at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac04fa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: file0.pdf\n",
      "file_path: data\\file0.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 328049\n",
      "creation_date: 2025-05-19\n",
      "last_modified_date: 2025-05-19\n",
      "\n",
      "arXiv:2505.10543v1  [cs.AI]  15 May 2025\n",
      "Towards a Deeper Understanding of Reasoning\n",
      "Capabilities in Large Language Models\n",
      "Annie Wong ,*, Thomas Bäck, Aske Plaat, Niki van Stein and Anna V . Kononova\n",
      "Leiden Institute of Advanced Computer Science\n",
      "Abstract. While large language models demonstrate impressive\n",
      "performance on static benchmarks, the true potential of large lan-\n",
      "guage models as self-learning and reasoning agents in dynamic envi-\n",
      "ronments remains unclear. This study systematically evaluates the ef-\n",
      "ficacy of self-reflection, heuristic mutation, and planning as prompt-\n",
      "ing techniques to test the adaptive capabilities of agents. We con-\n",
      "duct experiments with various open-source language models in dy-\n",
      "namic environments and find that larger models generally outperform\n",
      "smaller ones, but that strategic prompting can close this performance\n",
      "gap. Second, a too-long prompt can negatively impact smaller mod-\n",
      "els on basic reactive tasks, while larger models show more robust\n",
      "behaviour. Third, advanced prompting techniques primarily benefit\n",
      "smaller models on complex games, but offer less improvement for\n",
      "already high-performing large language models. Yet, we find that\n",
      "advanced reasoning methods yield highly variable outcomes: while\n",
      "capable of significantly improving performance when reasoning and\n",
      "decision-making align, they also introduce instability and can lead\n",
      "to big performance drops. Compared to human performance, our\n",
      "findings reveal little evidence of true emergent reasoning. Instead,\n",
      "large language model performance exhibits persistent limitations in\n",
      "crucial areas such as planning, reasoning, and spatial coordination,\n",
      "suggesting that current-generation large language models still suffer\n",
      "fundamental shortcomings that may not be fully overcome through\n",
      "self-reflective prompting alone. Reasoning is a multi-faceted task,\n",
      "and while reasoning methods like Chain of thought improves multi-\n",
      "step reasoning on math word problems, our findings using dynamic\n",
      "benchmarks highlight important shortcomings in general reasoning\n",
      "capabilities, indicating a need to move beyond static benchmarks to\n",
      "capture the complexity of reasoning.\n",
      "1 Introduction\n",
      "A key objective of artificial intelligence has been the development\n",
      "of intelligent agents that can perceive the environment and make au-\n",
      "tonomous decisions [31]. The emergence of large language models\n",
      "(LLMs) has significantly advanced this objective, showing strong ca-\n",
      "pabilities in the solution of various natural language processing tasks,\n",
      "such as solving mathematical problems, coding, reading comprehen-\n",
      "sion, translation, summarizing and answering questions [1, 4, 20].\n",
      "These findings show a promising direction towards autonomous\n",
      "agents, and recent studies have begun to explore methods for en-\n",
      "abling agents to learn dynamically. For instance, investigations into\n",
      "self-reflection mechanisms allow agents to evaluate their past actions\n",
      "and refine future strategies [22], while work on iterative prompting\n",
      "∗ Corresponding author. Email: a.s.w.wong@liacs.leidenuniv.nl\n",
      "and environmental feedback aims to facilitate continuous learning\n",
      "from experience [35]. LLMs have also been investigated to serve as\n",
      "a controller to solve environmental tasks through reasoning and plan-\n",
      "ning [33, 36, 19, 13]. The ability of LLM agents to learn and adapt in\n",
      "dynamic environments has yet to be definitively proven. While these\n",
      "models excel at in-context learning—generalizing from minimal ex-\n",
      "amples—their reliance on statistical prediction and lack of long-term\n",
      "memory often limit their effectiveness in dynamic settings [25]. Fur-\n",
      "thermore, achieving optimal performance on specialised tasks typi-\n",
      "cally requires either fine-tuning with human-annotated data [16] or\n",
      "reliance on careful prompt engineering [21]. These processes are\n",
      "resource-intensive, reducing the flexibility of deploying LLMs in\n",
      "real-world, constantly evolving applications. This study aims to un-\n",
      "derstand the following question: \" To what extent can LLM agents\n",
      "autonomously learn and adapt to novels tasks in dynamic environ-\n",
      "ments?\" Specifically, we investigate whether in-context mechanisms\n",
      "can improve continuous learning and multi-step reasoning across var-\n",
      "ious challenge levels. Our contributions are as follows 1:\n",
      "• We present a systematic comparison of open-souce LLMs on\n",
      "dynamic decision-making tasks. We quantify three prompting\n",
      "strategies—reflection, heuristic mutation , and planning—across\n",
      "SMART PLAY [32]: a benchmark to evaluate capabilities of intel-\n",
      "ligent agents. For simple reactive task, we find that excessive rea-\n",
      "soning harm performance for smaller models, while larger models\n",
      "are more robust. Larger models generally achieve higher scores in\n",
      "line with scaling laws [7], but carefully designed prompts can let\n",
      "small models match or surpass the baseline performance of larger\n",
      "models.\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1b5e91",
   "metadata": {},
   "source": [
    "A vector index store and a RAG pipeline are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df34d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ef8b5",
   "metadata": {},
   "source": [
    "Below is an example of using the meta data filters when make requests to the query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817df277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conclusion of the paper highlights the potential and limitations of advanced prompting strategies, such as self-reflection, heuristic mutation,\n",
      "and planning. It emphasizes that excessive reasoning can hinder the performance of smaller models on simple tasks by causing distractions and leading\n",
      "to overthinking, which can result in the model overlooking simpler and more effective solutions.\n"
     ]
    }
   ],
   "source": [
    "import textwrap \n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"7\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What is the conclusion of the paper?\", \n",
    ")\n",
    "\n",
    "wrapped_text = textwrap.fill(str(response), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b662ff7",
   "metadata": {},
   "source": [
    "Lets check the nodes used as content for the LLM to synthesize the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42840d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '7', 'file_name': 'file0.pdf', 'file_path': 'data\\\\file0.pdf', 'file_type': 'application/pdf', 'file_size': 328049, 'creation_date': '2025-05-19', 'last_modified_date': '2025-05-19'}\n",
      "{'page_label': '7', 'file_name': 'file0.pdf', 'file_path': 'data\\\\file0.pdf', 'file_type': 'application/pdf', 'file_size': 328049, 'creation_date': '2025-05-19', 'last_modified_date': '2025-05-19'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86239292",
   "metadata": {},
   "source": [
    "Below is an example of tooling the filter above for generic use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ef3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0397345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study on pages 1 and 2 focuses on evaluating the capabilities of large language models (LLMs) in dynamic environments through various prompting\n",
      "strategies. It compares different open-source LLMs on decision-making tasks using SMART PLAY as a benchmark. The study finds that carefully designed\n",
      "prompts can help smaller models match or exceed the performance of larger models. Advanced reasoning techniques can improve performance but may also\n",
      "introduce instability. Transforming sparse rewards into dense, task-aligned rewards enhances learning effectiveness. The study also notes limitations\n",
      "in self-learning and emergent reasoning in tasks requiring planning and coordination.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"o4-mini\", temperature=0)\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    \"Provide a summary of pages 1 and 2.\", \n",
    "    verbose=False\n",
    ")\n",
    "wrapped_text = textwrap.fill(str(response), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1b22b",
   "metadata": {},
   "source": [
    "Lets confirm only pages 1 and 2 were used as content for the synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71392454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'file0.pdf', 'file_path': 'data\\\\file0.pdf', 'file_type': 'application/pdf', 'file_size': 328049, 'creation_date': '2025-05-19', 'last_modified_date': '2025-05-19'}\n",
      "{'page_label': '1', 'file_name': 'file0.pdf', 'file_path': 'data\\\\file0.pdf', 'file_type': 'application/pdf', 'file_size': 328049, 'creation_date': '2025-05-19', 'last_modified_date': '2025-05-19'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
