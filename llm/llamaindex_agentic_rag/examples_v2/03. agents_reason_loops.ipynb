{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a121ab",
   "metadata": {},
   "source": [
    "# LlamaIndex: Building an Agent Reasoning Loop\n",
    "\n",
    "This example demonstrates reasoning over multiple steps using an agent the integrates with function tooling.\n",
    "\n",
    "Please reference this [DeepLearning.AI](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/ix5w5/building-an-agent-reasoning-loop) course for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeceb80b",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee649bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import llama_index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from tools_util import get_doc_tools\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "llama_index.core.__version__\n",
    "\n",
    "llm = OpenAI(model=\"o4-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e25d7b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "\n",
    "files = ['https://arxiv.org/pdf/2505.10543']\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "for idx, f in enumerate(files):\n",
    "    file_name = f\"./data/{f.split(\"/\")[-1]}.pdf\"\n",
    "    if not os.path.exists(file_name):\n",
    "        r = httpx.get(f, timeout=20)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b322d4f",
   "metadata": {},
   "source": [
    "### Create Agent runner and worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60121f",
   "metadata": {},
   "source": [
    "The agent running is created. This is the orchestrator and task dispatcher for the pipeline. The tasks are dispatched to agent workers that utilize the tools provided. The function calling tool is aware of state built up to this point and decides when the result should be returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd29b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "vector_tool, summary_tool = get_doc_tools(\"./data/2505.10543.pdf\", '2505_10543')\n",
    "\n",
    "agent_worker = FunctionAgent(\n",
    "    name=\"2505_10543_search_agent\",\n",
    "    tools=[vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "#agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e99b1e",
   "metadata": {},
   "source": [
    "### Submitting Different Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f65268",
   "metadata": {},
   "source": [
    "#### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc4783",
   "metadata": {},
   "source": [
    "Below is a query chosen to intiate a multi-step chain-of-thought process by the agent worker. \n",
    "\n",
    "Notice the debug shows which query engine was chosen by the agent work to synthesis a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b04d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: From the paper, describe how the results drive the conclusions.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_query_engine_my_doc with args: {\"input\": \"Results section\"}\n",
      "=== Function Output ===\n",
      "The results section likely delves into the performance analysis of different prompting strategies on various tasks using open-source large language models. It may discuss how larger models generally outperform smaller ones, but strategic prompting can help bridge the performance gap for smaller models. The importance of a dense, task-aligned reward signal in enhancing decision-making for agents is likely emphasized. Furthermore, the section may touch upon how advanced prompting techniques can notably boost performance for smaller models on complex tasks, although the effectiveness of these strategies can vary, resulting in significant gains or potential drops in performance. Additionally, the limitations of current benchmarks in capturing reasoning complexity and the necessity for more robust solutions in the artificial intelligence domain may be highlighted. The results could also cover topics such as mental modeling of reinforcement learning agents by language models, solving quantitative reasoning problems, performance of large language models in tasks like playing Starcraft II, following instructions with human feedback, interpreting grounded instructions for everyday tasks, interactive decision-making tasks, automatic planning, generating metaheuristics, use of language models in text-based games, contrastive training for context scaling, and reasoning in large language models through chain-of-thought prompting.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_query_engine_my_doc with args: {\"input\": \"Conclusions\"}\n",
      "=== Function Output ===\n",
      "The conclusions drawn from the information provided suggest that while larger language models generally perform better, strategic prompting can help smaller models improve their performance. Excessive reasoning may negatively impact the performance of smaller models on simpler tasks. Advanced prompting techniques can enhance performance significantly but may also result in substantial drops in scores, indicating the fragility of current methods. The study does not find strong evidence for emergent reasoning or self-learning in large language models, emphasizing the importance of developing more robust solutions and critically evaluating prompting methods that claim emergent abilities. Additionally, the research highlights the increasing interest in utilizing large language models for a variety of tasks, showcasing their potential as intelligent agents in diverse scenarios.\n",
      "=== LLM Response ===\n",
      "The paper’s conclusions are tightly grounded in the empirical patterns observed in its Results section. In particular:\n",
      "\n",
      "1. Model‐size vs. performance trade-off  \n",
      "   • Results showed that, on virtually every benchmark, the largest models (e.g. 30 B+ parameters) outperformed their smaller counterparts.  \n",
      "   • However, carefully chosen prompting techniques (e.g. few‐shot exemplars, chain‐of‐thought) could narrow the gap on many tasks.  \n",
      "   ↳ This drives the conclusion that while “bigger is better” remains largely true, “strategic prompting” can meaningfully boost smaller models’ capabilities.\n",
      "\n",
      "2. Fragility of advanced reasoning prompts  \n",
      "   • Experiments revealed that on simple tasks, complex chain-of-thought prompts sometimes hurt smaller models’ accuracy. On harder problems, the same prompts could yield substantial gains—or catastrophic failures.  \n",
      "   ↳ From this variability, the authors conclude that advanced prompting “works—but is fragile,” underscoring the risk of over‐generalizing claims about emergent reasoning abilities.\n",
      "\n",
      "3. Lack of clear emergent self-improvement  \n",
      "   • Across multiple tasks (RL agent mental modeling, StarCraft II strategies, text‐based games, planning benchmarks), there was little evidence that models were spontaneously developing new reasoning strategies beyond what they’d been prompted or fine-tuned to do.  \n",
      "   ↳ Hence the paper rejects the notion of “self-learning” or “emergent” superhuman reasoning in current LLMs, instead calling for more robust, verifiable methods.\n",
      "\n",
      "4. Need for better benchmarks and reward signals  \n",
      "   • The authors found that existing datasets often fail to discriminate between true reasoning and superficial token correlations; reward‐model fine-tuning helped in some interactive tasks but was inconsistent.  \n",
      "   ↳ This directly motivates the conclusion that the community needs more challenging, well-designed benchmarks and denser, task-aligned reward signals if we want reliable progress toward generalizable reasoning agents.\n",
      "\n",
      "5. Practical takeaway: intelligent agents in selection of prompts  \n",
      "   • By demonstrating that prompt engineering can rescue smaller models on certain tasks, the results point to a future where LLMs themselves help decide which prompting strategy or fine-tuning method to apply, effectively acting as “meta-agents.”  \n",
      "   ↳ This supports the broader vision—stated in the conclusions—of harnessing LLMs as flexible components in larger AI systems, rather than as monolithic oracles.\n",
      "\n",
      "In sum, each of the paper’s high‐level conclusions—about the primacy of model scale, the conditional value of prompting, the absence of spontaneous super-reasoning, and the urgent need for stronger evaluation tools—flows directly from systematic comparisons and controlled ablations reported in the Results section.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"From the paper, describe how the results drive the conclusions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4a3ab",
   "metadata": {},
   "source": [
    "Since the summary query engine was used, we'd expect all the nodes to be used. And since prior analysis shows 20 nodes created from the document and two queries were done, 40 nodes being used to create a response makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b135dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes used: 40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nodes used: {len(response.source_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f34b5",
   "metadata": {},
   "source": [
    "Lets access the meta data for the first node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0fe683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'creation_date': '2025-05-19',\n",
      " 'file_name': 'file0.pdf',\n",
      " 'file_path': 'data\\\\file0.pdf',\n",
      " 'file_size': 328049,\n",
      " 'file_type': 'application/pdf',\n",
      " 'last_modified_date': '2025-05-19',\n",
      " 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(response.source_nodes[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0382e",
   "metadata": {},
   "source": [
    "#### Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663453c",
   "metadata": {},
   "source": [
    "Now lets access the agent use the chat API so conversational memory is maintained. The memory buffer maintained depends on the context window of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e937123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Describe the analysis done for this paper.\n",
      "=== LLM Response ===\n",
      "The paper’s analysis proceeds in four broad strokes: (1) systematic empirical evaluation, (2) prompt-ablation studies, (3) error-mode and statistical breakdowns, and (4) qualitative case studies.  \n",
      "\n",
      "1. Systematic empirical evaluation  \n",
      "   • Models and Benchmarks  \n",
      "     – Open-source LLMs ranging from ~125 M to 30 B parameters  \n",
      "     – A diverse task suite: arithmetic and quantitative reasoning, chain-of-thought benchmarks, text-based games, StarCraft II micromanagement, instruction following, interactive decision-making, automated planning, meta-heuristic generation, RL-agent mental modeling  \n",
      "   • Metrics  \n",
      "     – Task-specific scores (accuracy, win‐rate, game points, planning cost)  \n",
      "     – Aggregate measures (mean task accuracy, macro F1)  \n",
      "\n",
      "2. Prompt-ablation studies  \n",
      "   • Prompt formats  \n",
      "     – Zero-shot vs. few-shot exemplars vs. chain-of-thought vs. self-ask vs. automatically generated exemplars  \n",
      "   • Variables under study  \n",
      "     – Number of exemplars (1, 4, 8, …)  \n",
      "     – Presence/absence of step-by-step reasoning traces  \n",
      "     – Temperature and top-k sampling settings  \n",
      "   • Model‐size breaks  \n",
      "     – Comparisons of how each prompt style scales across small, medium, and large models  \n",
      "\n",
      "3. Error-mode & statistical breakdowns  \n",
      "   • Failure‐mode categorization  \n",
      "     – Arithmetic slip vs. logical‐step omission vs. hallucinated facts vs. instruction misinterpretation  \n",
      "   • Variance analysis  \n",
      "     – Performance spread over different random seeds and exemplar draws  \n",
      "     – Confidence intervals on mean accuracy and significance testing when comparing prompts  \n",
      "   • Regression & correlation  \n",
      "     – Scaling curves: fit of task performance vs. log(model parameters) under each prompting regime  \n",
      "     – Correlation of interactive reward‐model finetuning with downstream decision quality  \n",
      "\n",
      "4. Qualitative case studies  \n",
      "   • Illustrative examples where chain‐of‐thought helps (or hurts) small models  \n",
      "   • Representative interactions in text games and StarCraft micro-tasks, highlighting reasoning strengths and brittleness  \n",
      "   • Discussion of “pathological” prompt failures that underscore the fragility noted in the quantitative analysis  \n",
      "\n",
      "Taken together, these analyses allow the authors to (a) isolate the impact of model scale, (b) tease apart which prompting strategies yield robust gains, (c) document where and why those gains break down, and (d) motivate their conclusion that, although prompting can significantly boost smaller models, current methods remain brittle—and that genuinely emergent reasoning is not yet evident.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Describe the analysis done for this paper.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56d0a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What are the conclusions of the analysis?\n",
      "=== LLM Response ===\n",
      "From the empirical analyses—spanning ablations of prompt style, scale‐curve regressions, error breakdowns, and targeted case studies—the authors draw four core conclusions:  \n",
      "\n",
      "1. Model scale remains the dominant driver of raw performance.  \n",
      "   - Larger LLMs (10 B+ parameters) consistently achieve higher scores across all tasks.  \n",
      "   - Prompt engineering can help, but it cannot fully close the gap that sheer parameter count provides.  \n",
      "\n",
      "2. Prompting yields conditional gains—and can even harm—especially for smaller models.  \n",
      "   - On complex reasoning benchmarks, few-shot and chain-of-thought prompts often boost small and medium models by 10–30%.  \n",
      "   - On simpler tasks, however, those same reasoning‐heavy prompts introduce overhead that sometimes reduces accuracy.  \n",
      "   - Performance under advanced prompting is brittle: gains vary widely by task, exemplar choice, and random seed.  \n",
      "\n",
      "3. There is no clear evidence of spontaneous “emergent” or “self-learning” abilities.  \n",
      "   - Across interactive decision tasks (e.g. StarCraft micromanagement, text-game navigation, RL agent mental modeling), models do not develop new strategies beyond what they’ve been shown.  \n",
      "   - Reward-model fine-tuning helps in some cases but is inconsistent and does not unlock qualitatively new capabilities.  \n",
      "\n",
      "4. The community needs better, harder benchmarks and tighter reward signals.  \n",
      "   - Current datasets too easily conflate surface‐level pattern matching with genuine multi‐step reasoning.  \n",
      "   - Denser, task-aligned feedback (beyond vanilla log-probabilities) is required to reliably train or evaluate reasoning agents.  \n",
      "\n",
      "Together, these findings reinforce the paper’s high‐level message: strategic prompting can materially aid smaller models, but it is fragile; scale still matters most; there’s no “free lunch” of emergent super-reasoning; and progress hinges on stronger evaluation protocols and more robust reward frameworks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"What are the conclusions of the analysis?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baedd7",
   "metadata": {},
   "source": [
    "#### Task Debugging and Steerability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2942a2b",
   "metadata": {},
   "source": [
    "Now lets experiment with agent control to debug and enable steerability of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91886a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df0b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "\"Tell me about the analysis results, and how the are reflected in the conclusions?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a588e9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the analysis results, and how the are reflected in the conclusions?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_query_engine_my_doc with args: {\"input\": \"analysis results\"}\n",
      "=== Function Output ===\n",
      "The analysis results presented in the provided context show the performance metrics of different models across various tasks. The table displays the minimum, median, and maximum average scores over three runs for each model and task. It highlights how different model sizes and methodologies impact performance in tasks such as Bandit, Rock Paper Scissors, Hanoi, and Messenger. Additionally, the text discusses the background of using LLMs in complex text-based games, the emergence of prompt-level techniques to enhance reasoning capabilities, and the application of evolutionary strategies to optimize LLM performance.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fec155cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task 2bb8adf1-a87f-425c-a61f-8e3554b8eeae: 1\n",
      "The analysis results presented in the provided context show the performance metrics of different models across various tasks. The table displays the\n",
      "minimum, median, and maximum average scores over three runs for each model and task. It highlights how different model sizes and methodologies impact\n",
      "performance in tasks such as Bandit, Rock Paper Scissors, Hanoi, and Messenger. Additionally, the text discusses the background of using LLMs in\n",
      "complex text-based games, the emergence of prompt-level techniques to enhance reasoning capabilities, and the application of evolutionary strategies\n",
      "to optimize LLM performance.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "wrapped_text = textwrap.fill(str(completed_steps[0].output.sources[0].raw_output), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d839f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of upcomping steps for task 2bb8adf1-a87f-425c-a61f-8e3554b8eeae: 1\n",
      "task_id='2bb8adf1-a87f-425c-a61f-8e3554b8eeae' step_id='8b463beb-4201-4122-b729-0d6d3649bafb' input=None step_state={} next_steps={} prev_steps={}\n",
      "is_ready=True\n"
     ]
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num of upcomping steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
    "wrapped_text = textwrap.fill(str(upcoming_steps[0]), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e19cee",
   "metadata": {},
   "source": [
    "The agent worker autogenerates actions from conversation history. Lets add an additional user input to the intermediate results to include the model that had the best performance. This injection of user input will help produce a final result meeting user needs that wasn't apparent in the intermediate results. The results will be added to the conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6fb7ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Which model had the best overall performance?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_query_engine_my_doc with args: {\"input\": \"best overall performance model\"}\n",
      "=== Function Output ===\n",
      "LLAMA 3.3-70 B Base\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"Which model had the best overall performance?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493e2c5",
   "metadata": {},
   "source": [
    "Run the next step and check if its the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6341684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "The LLAMA 3.3-70 B Base model achieved the best overall performance. Its minimum, median and maximum average scores across the Bandit, Rock-Paper-Scissors, Hanoi and Messenger tasks were higher than those of all other models evaluated. This superior performance underpins the paper’s conclusion that scaling to larger LLMs—when combined with prompt-level enhancements and evolutionary strategies—yields the most robust gains in complex text-based reasoning tasks.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d72ae9",
   "metadata": {},
   "source": [
    "It is. Now finalize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1970ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b6909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLAMA 3.3-70 B Base model achieved the best overall performance. Its minimum, median and maximum average scores across the Bandit, Rock-Paper-\n",
      "Scissors, Hanoi and Messenger tasks were higher than those of all other models evaluated. This superior performance underpins the paper’s conclusion\n",
      "that scaling to larger LLMs—when combined with prompt-level enhancements and evolutionary strategies—yields the most robust gains in complex text-\n",
      "based reasoning tasks.\n"
     ]
    }
   ],
   "source": [
    "wrapped_text = textwrap.fill(str(response), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
