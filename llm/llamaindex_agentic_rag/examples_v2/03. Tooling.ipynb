{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a121ab",
   "metadata": {},
   "source": [
    "# LlamaIndex: Tooling Examples\n",
    "\n",
    "This example demonstrates reasoning over multiple steps using an agent the integrates with function tooling.\n",
    "\n",
    "Please reference this [DeepLearning.AI](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/ix5w5/building-an-agent-reasoning-loop) course for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeceb80b",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee649bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import llama_index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import textwrap\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "from tools_util import get_doc_tools\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "llama_index.core.__version__\n",
    "\n",
    "llm = OpenAI(model=\"o4-mini\", temperature=0)\n",
    "\n",
    "def long_print(msg: str):\n",
    "    wrapped_text = textwrap.fill(msg, width=140, replace_whitespace=False)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e25d7b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75be2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "\n",
    "files = ['https://arxiv.org/pdf/2505.10543']\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "for idx, f in enumerate(files):\n",
    "    file_name = f\"./data/{f.split(\"/\")[-1]}.pdf\"\n",
    "    if not os.path.exists(file_name):\n",
    "        r = httpx.get(f, timeout=20)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b322d4f",
   "metadata": {},
   "source": [
    "### Creating Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fc4c2",
   "metadata": {},
   "source": [
    "#### Function Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d420dec",
   "metadata": {},
   "source": [
    "Below, some basic tooling is done using the `FunctionAgent` class to decide which tool to call.\n",
    "\n",
    "Note that I purposely implemented the `multiply` tool incorrectly to ensure that the tool is being called. I have noticed that sometimes the LLM will not call the tool and produce an answer that doesn't reflect the tool implementations.\n",
    "\n",
    "To address this, I found it necessary to add system prompts and an output schema to ensure usable responses. These additions did not completely resolve the problems. Sometimes the asserts in the script below fail and the tools provided to the agent are not called. These additions have also considerably added to the response time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b4561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition response: 9\n",
      "Multiply response: 1024\n"
     ]
    }
   ],
   "source": [
    "class IntegerResult(BaseModel):\n",
    "    \"\"\"Data model for a tool output.\"\"\"\n",
    "    result: int\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"A tool to add two integers together.\"\"\"\n",
    "    print('Hit add numbers.')\n",
    "    return x + y\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"A tool to multiply two integers together.\"\"\"\n",
    "    print('Hit multiply numbers.')\n",
    "    return x ** y\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add, fn_schema=IntegerResult)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply, fn_schema=IntegerResult)\n",
    "\n",
    "agent1 = FunctionAgent(\n",
    "    name=\"add_multiply_agent\",\n",
    "    description=\"An agent that can add or multiply two numbers.\",\n",
    "    tools=[add_tool, multiply_tool], \n",
    "    llm=llm, \n",
    "    system_prompt=(\"Only use the available tools to answer questions.\"\n",
    "                   \"Do not evaluate output.\"\n",
    "                   \"The final response should only contain tool output.\"),\n",
    "    output_parser=PydanticOutputParser(output_cls=IntegerResult),\n",
    "    verbose=True\n",
    ")\n",
    "response = await agent1.run('Add the numbers 4 and 5.')\n",
    "print(f'Addition response: {str(response)}')\n",
    "assert int(str(response)) == 9\n",
    "response = await agent1.run('Multiply the numbers 4 and 5.')\n",
    "print(f'Multiply response: {str(response)}')\n",
    "assert int(str(response)) == 4 ** 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60121f",
   "metadata": {},
   "source": [
    "#### Query Engine Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca3910",
   "metadata": {},
   "source": [
    "The `get_doc_tools` module function below creates two query engines tools: one that specializes in specific question about a document and another that specializes in summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097443d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool, summary_tool = get_doc_tools(\"./data/2505.10543.pdf\", '2505_10543')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b641c",
   "metadata": {},
   "source": [
    "The summary tool used to get a summary of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ab13be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses the evaluation of large language models on dynamic decision-making tasks and the effectiveness of strategic prompting\n",
      "techniques in closing the performance gap between larger and smaller models, emphasizing the limitations of current models in emergent\n",
      "reasoning and self-learning.\n"
     ]
    }
   ],
   "source": [
    "summarization_response = summary_tool.call(\"Please provide a concise sentence that summarizes the document.\")\n",
    "long_print(str(summarization_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f57842",
   "metadata": {},
   "source": [
    "Now lets ask a more specific question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c60d6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conclusions of the document are as follows:\n",
      "- Larger language models generally achieve higher scores in line with scaling laws, but\n",
      "carefully designed prompts can enable smaller models to match or surpass the baseline performance of larger models.\n",
      "- Advanced prompting\n",
      "techniques primarily benefit smaller models on complex planning and reasoning tasks, offering less value to already high-performing large\n",
      "language models.\n",
      "- Advanced reasoning techniques can significantly improve performance when reasoning and decision-making align, but they\n",
      "can also introduce instability and lead to significant performance drops.\n",
      "- Transforming a sparse reward into a dense, task-aligned\n",
      "quantitative reward can improve the learning effectiveness of large language model agents within complex environments.\n",
      "- There is little\n",
      "evidence for self-learning or emergent reasoning in dynamic tasks that require planning and spatial coordination, highlighting the\n",
      "limitations of current static approaches.\n"
     ]
    }
   ],
   "source": [
    "detailed_response = vector_tool.call(\"What are the conclusions of the document?\")\n",
    "long_print(str(detailed_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf799d6",
   "metadata": {},
   "source": [
    "### Multi-Tool Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba457c42",
   "metadata": {},
   "source": [
    "Now lets combine all the tools created above into one agent to see if correct tools are utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4a781a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = FunctionAgent(\n",
    "    name=\"A multi-tool agent\",\n",
    "    description=\"An agent that perform limited arithmetic and answer certain LLM questions.\",\n",
    "    tools=[add_tool, multiply_tool, vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    system_prompt=(\"Only use your available tools to answer questions.\"\n",
    "                   \"Do not evaluate output.\"\n",
    "                   \"The final response should only contain tool output.\"),\n",
    "    verbose=True\n",
    ")\n",
    "response = await agent2.run(\"What conclusions can be drawn about LLM reasoning. Also, multiply 4 and 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5fade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conclusions about LLM reasoning:\n",
      "- Larger models generally exhibit better reasoning performance.\n",
      "- Strategic prompting techniques can\n",
      "help smaller models close the gap but may also cause overthinking in simple tasks.\n",
      "- Advanced prompting can boost smaller models on complex\n",
      "tasks, though results are inconsistent and can sometimes worsen performance.\n",
      "- LLMs currently show limitations in emergent reasoning or\n",
      "self-learning, often failing by producing invalid action sequences or getting stuck in loops.\n",
      "- There is a need for more robust solutions\n",
      "and critical assessment of prompting methods that claim to enable advanced LLM capabilities.\n",
      "\n",
      "Result of multiplying 4 and 5: 1024\n"
     ]
    }
   ],
   "source": [
    "long_print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb695cf1",
   "metadata": {},
   "source": [
    "The response above reflects the content of the document and the purposely-defective multiply tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e2d29",
   "metadata": {},
   "source": [
    "Below are the tools called by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ad040ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_query_engine_2505_10543\n",
      "multiply\n"
     ]
    }
   ],
   "source": [
    "for tool in response.tool_calls:\n",
    "    print(tool.tool_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba14b7",
   "metadata": {},
   "source": [
    "Note, the response is not deterministic. Sometimes the multiply tool was not called to answer the arithmetic part of the questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
