{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a121ab",
   "metadata": {},
   "source": [
    "# LlamaIndex: Tooling Examples\n",
    "\n",
    "This example demonstrates reasoning over multiple steps using an agent the integrates with function tooling.\n",
    "\n",
    "Please reference this [DeepLearning.AI](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/ix5w5/building-an-agent-reasoning-loop) course for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeceb80b",
   "metadata": {},
   "source": [
    "### Setup The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee649bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import llama_index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import textwrap\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "from tools_util import get_doc_tools\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "llama_index.core.__version__\n",
    "\n",
    "llm = OpenAI(model=\"o4-mini\", temperature=0)\n",
    "\n",
    "def long_print(msg: str):\n",
    "    wrapped_text = textwrap.fill(msg, width=140, replace_whitespace=False)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e25d7b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75be2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "\n",
    "files = ['https://arxiv.org/pdf/2505.10543']\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "for idx, f in enumerate(files):\n",
    "    file_name = f\"./data/{f.split(\"/\")[-1]}.pdf\"\n",
    "    if not os.path.exists(file_name):\n",
    "        r = httpx.get(f, timeout=20)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b322d4f",
   "metadata": {},
   "source": [
    "### Creating Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fc4c2",
   "metadata": {},
   "source": [
    "#### Function Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d420dec",
   "metadata": {},
   "source": [
    "Below, some basic tooling is done using the `FunctionAgent` class to decide which tool to call.\n",
    "\n",
    "Note that I purposely implemented the `multiply` tool incorrectly to ensure that the tool is being called. I have noticed that sometimes the LLM will not call the tool and produce an answer that doesn't reflect the tool implementations.\n",
    "\n",
    "To address this, I found it necessary to add system prompts and an output schema to ensure usable responses. These additions did not completely resolve the problems. Sometimes the asserts in the script below fail and the tools provided to the agent are not called. These additions have also considerably added to the response time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4561d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit add numbers.\n",
      "Addition response: 9\n",
      "Hit multiply numbers.\n",
      "Multiply response: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit multiply numbers.\n"
     ]
    }
   ],
   "source": [
    "class IntegerResult(BaseModel):\n",
    "    \"\"\"Data model for a tool output.\"\"\"\n",
    "    result: int\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"A tool to add two integers together.\"\"\"\n",
    "    print('Hit add numbers.')\n",
    "    return x + y\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"A tool to multiply two integers together.\"\"\"\n",
    "    print('Hit multiply numbers.')\n",
    "    return x ** y\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add, fn_schema=IntegerResult)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply, fn_schema=IntegerResult)\n",
    "\n",
    "agent1 = FunctionAgent(\n",
    "    name=\"add_multiply_agent\",\n",
    "    description=\"An agent that can add or multiply two numbers.\",\n",
    "    tools=[add_tool, multiply_tool], \n",
    "    llm=llm, \n",
    "    system_prompt=(\"Only use the available tools to answer questions.\"\n",
    "                   \"Do not evaluate output.\"\n",
    "                   \"The final response should only contain tool output.\"),\n",
    "    output_parser=PydanticOutputParser(output_cls=IntegerResult),\n",
    "    verbose=True\n",
    ")\n",
    "response = await agent1.run('Add the numbers 4 and 5.')\n",
    "print(f'Addition response: {str(response)}')\n",
    "assert int(str(response)) == 9\n",
    "response = await agent1.run('Multiply the numbers 4 and 5.')\n",
    "print(f'Multiply response: {str(response)}')\n",
    "assert int(str(response)) == 4 ** 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60121f",
   "metadata": {},
   "source": [
    "#### Query Engine Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca3910",
   "metadata": {},
   "source": [
    "The `get_doc_tools` module function below creates two query engines tools: one that specializes in specific question about a document and another that specializes in summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097443d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool, summary_tool = get_doc_tools(\"./data/2505.10543.pdf\", '2505_10543')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b641c",
   "metadata": {},
   "source": [
    "The summary tool used to get a summary of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab13be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document explores the evaluation and utilization of large language models in dynamic decision-making tasks, emphasizing the impact of\n",
      "model size, prompting strategies, and reward shaping on their performance in complex scenarios, revealing both potential and limitations in\n",
      "reasoning capabilities.\n"
     ]
    }
   ],
   "source": [
    "summarization_response = summary_tool.call(\"Please provide a concise sentence that summarizes the document.\")\n",
    "long_print(str(summarization_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f57842",
   "metadata": {},
   "source": [
    "Now lets ask a more specific question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c60d6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conclusions of the document are that advanced prompting techniques can help smaller models match or surpass the performance of larger\n",
      "models on complex planning and reasoning tasks. Additionally, advanced reasoning techniques can significantly improve performance when\n",
      "reasoning and decision-making align, but they can also introduce instability and lead to performance drops. Transforming sparse rewards into\n",
      "dense, task-aligned quantitative rewards can improve the learning effectiveness of LLM agents in complex environments. The document also\n",
      "notes the limitations of current static approaches in tasks that require planning and spatial coordination.\n"
     ]
    }
   ],
   "source": [
    "detailed_response = vector_tool.call(\"What are the conclusions of the document?\")\n",
    "long_print(str(detailed_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf799d6",
   "metadata": {},
   "source": [
    "### Multi-Tool Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba457c42",
   "metadata": {},
   "source": [
    "Now lets combine all the tools created above into one agent to see if correct tools are utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a781a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = FunctionAgent(\n",
    "    name=\"A multi-tool agent\",\n",
    "    description=\"An agent that perform limited arithmetic and answer certain LLM questions.\",\n",
    "    tools=[add_tool, multiply_tool, vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    system_prompt=(\"Only use your available tools to answer questions.\"\n",
    "                   \"Do not evaluate output.\"\n",
    "                   \"The final response should only contain tool output.\"),\n",
    "    verbose=True\n",
    ")\n",
    "response = await agent2.run(\"What conclusions can be drawn about LLM reasoning. Also, multiply 4 and 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5fade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conclusions about LLM reasoning:\n",
      "- Larger models generally exhibit stronger reasoning abilities.\n",
      "- Strategic prompting (self-reflection,\n",
      "heuristic mutation, planning) can help smaller models improve but with inconsistent results.\n",
      "- Over-reasoning can harm smaller modelsâ€™\n",
      "performance on simple tasks.\n",
      "- Current LLMs have limitations in emergent reasoning and self-learning, often hallucinating or looping.\n",
      "-\n",
      "There is a need for more robust solutions and careful evaluation of prompting strategies.\n",
      "\n",
      "Product of 4 and 5:\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "long_print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb695cf1",
   "metadata": {},
   "source": [
    "The response above reflects the content of the document and the purposely-defective multiply tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e2d29",
   "metadata": {},
   "source": [
    "Below are the tools called by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad040ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_query_engine_2505_10543\n",
      "multiply\n"
     ]
    }
   ],
   "source": [
    "for tool in response.tool_calls:\n",
    "    print(tool.tool_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba14b7",
   "metadata": {},
   "source": [
    "Note, the response is not deterministic. Sometimes the multiply tool was not called to answer the arithmetic part of the questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
