{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a121ab",
   "metadata": {},
   "source": [
    "# LlamaIndex: Building an Agent Reasoning Loop\n",
    "\n",
    "This example demonstrates reasoning over multiple steps using an agent the integrates with function tooling.\n",
    "\n",
    "Please reference this [DeepLearning.AI](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/ix5w5/building-an-agent-reasoning-loop) course for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeceb80b",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee649bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import llama_index\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import textwrap\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from tools_util import get_doc_tools\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "llama_index.core.__version__\n",
    "\n",
    "llm = OpenAI(model=\"o4-mini\", temperature=0)\n",
    "\n",
    "def long_print(msg: str):\n",
    "    wrapped_text = textwrap.fill(msg, width=140, replace_whitespace=False)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e25d7b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75be2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import os\n",
    "\n",
    "files = ['https://arxiv.org/pdf/2505.10543']\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "for idx, f in enumerate(files):\n",
    "    file_name = f\"./data/{f.split(\"/\")[-1]}.pdf\"\n",
    "    if not os.path.exists(file_name):\n",
    "        r = httpx.get(f, timeout=20)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b322d4f",
   "metadata": {},
   "source": [
    "### Creating An Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe60121f",
   "metadata": {},
   "source": [
    "The agent running is created. This is the orchestrator and task dispatcher for the pipeline. The tasks are dispatched to agent workers that utilize the tools provided. The function calling tool is aware of state built up to this point and decides when the result should be returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097443d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tool, summary_tool = get_doc_tools(\"./data/2505.10543.pdf\", '2505_10543')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab13be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_response = summary_tool.call(\"Please provide a concise sentence that summarizes the document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3481d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document explores the evaluation of large language models on dynamic decision-making tasks, emphasizing the performance differences\n",
      "between larger and smaller models and the impact of strategic prompting techniques on bridging this gap, while also highlighting the\n",
      "limitations of current models in emergent reasoning and self-learning.\n"
     ]
    }
   ],
   "source": [
    "long_print(str(summarization_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdd29b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = FunctionAgent(\n",
    "    name=\"2505_10543_search_agent\",\n",
    "    description=str(summarization_response),\n",
    "    tools=[vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    system_prompt=\"You are a helpful assistant that can answer questions about a document.\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4879d450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document presents a study on how well large language models (LLMs) perform reasoning tasks in dynamic environments. It examines and\n",
      "compares the decision-making abilities of several open-source LLMs under different prompting strategies. Key points include:\n",
      "\n",
      "• Impact of\n",
      "model size: Larger models generally perform better at dynamic reasoning tasks.  \n",
      "• Effectiveness of prompting: Techniques like chain-of-\n",
      "thought, self-consistency, and reflective prompting can bridge some performance gaps but have limits.  \n",
      "• Current limitations: LLMs still\n",
      "struggle with planning, spatial coordination, and long-horizon reasoning.  \n",
      "• Need for robust solutions: The study argues for approaches\n",
      "beyond prompting—such as better model architectures or learning paradigms—to overcome these shortcomings.  \n",
      "• Broader research context: It\n",
      "also surveys related work on quantitative reasoning, reinforcement learning with language models, video-game playing, instruction following,\n",
      "and automated planning.\n",
      "\n",
      "Overall, the document highlights both the promise and the present limitations of using LLMs in dynamic, decision-\n",
      "making settings and calls for more advanced techniques to achieve reliable reasoning and self-learning.\n"
     ]
    }
   ],
   "source": [
    "response = await agent1.run(user_msg=\"What is the document about?\")\n",
    "long_print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5d4d0",
   "metadata": {},
   "source": [
    "An agentic workflow is created with only one agent. The results should be similar. AgentWorkflow is an orchestrator for agents. A good introduction can be found [here](https://www.llamaindex.ai/blog/introducing-agentworkflow-a-powerful-system-for-building-ai-agent-systems). Per the intro, \"AgentWorkflow builds on top of our popular Workflow abstractions to make agents even easier to get up and running. AgentWorkflows take care of coordinating between agents, maintaining state, and more, while still giving you the flexibility and extensibility of Workflows.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395df444",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = AgentWorkflow(agents=[agent1])\n",
    "response = await workflow.run(user_msg=\"What is the document about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44fee319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document examines how large language models perform in dynamic, decision-making environments and evaluates various prompting strategies\n",
      "(such as self-reflection, heuristic mutation, and planning) to improve their reasoning. It compares models of different sizes on the SMART\n",
      "PLAY benchmark, highlights gaps between smaller and larger models, and studies the effectiveness of advanced prompts across tasks. The paper\n",
      "also surveys related work on using language models for quantitative reasoning, reinforcement-learning agent modeling, video-game play,\n",
      "grounded instruction following, instruction fine-tuning, prompt programming, text-based game benchmarks, contrastive training, and LLM-based\n",
      "agents—while emphasizing current limitations in planning, reasoning, spatial coordination, and self-learning.\n"
     ]
    }
   ],
   "source": [
    "wrapped_text = textwrap.fill(str(response), width=140, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35927aa3",
   "metadata": {},
   "source": [
    "### Creating a Multi-Agent System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82215ddc",
   "metadata": {},
   "source": [
    "Lets more fully utilize the `AgentWorkflow` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1b4561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IntegerResult(BaseModel):\n",
    "    \"\"\"Data model for a tool output.\"\"\"\n",
    "    result: int\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    print('Hit add numbers.')\n",
    "    return x + y\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    print('Hit multiply numbers.')\n",
    "    return x ** y ** 2\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add, description=\"A tool to add two integers together.\")\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply, description=\"A tool to multiply two integers together.\")\n",
    "\n",
    "agent2 = FunctionAgent(\n",
    "    name=\"add_multiply_agent\",\n",
    "    description=\"An agent that can add or multiply two numbers.\",\n",
    "    tools=[add_tool, multiply_tool], \n",
    "    llm=llm, \n",
    "    system_prompt=(\"Only use the available tools to answer questions. \"\n",
    "                   \"Do not evaluate output.\"\n",
    "                   \"The final response should only contain tool output.\"),\n",
    "    output_parser=PydanticOutputParser(output_cls=IntegerResult),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c60d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent2.run('Add the numbers 4 and 5.')\n",
    "assert int(str(response)) == 9\n",
    "response = await agent2.run('Multiply the numbers 4 and 5.')\n",
    "assert int(str(response)) == 4 ** 5 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = AgentWorkflow(agents=[agent1, agent2], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ebcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await workflow.run(user_msg=\"Multiply 5 and 6 and explain to me the limitations of LLM advanced prompting strategies.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
