{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5deb1d8",
   "metadata": {},
   "source": [
    "### LlamaIndex: Router Query Engine\n",
    "\n",
    "Here, I experiment with LlamaIndex routers. \n",
    "\n",
    "Please reference this [DeepLearning.AI](https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/it0jz/router-query-engine) course for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a8a4d",
   "metadata": {},
   "source": [
    "### Load Environment\n",
    "\n",
    "Load environment variables and allow asyncio to be used in the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be655885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.36'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import textwrap \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import llama_index.core\n",
    "llama_index.core.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafc5f1",
   "metadata": {},
   "source": [
    "### Retrieve and Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10040d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes after splitting: 20\n",
      "page_label: 1\n",
      "file_name: 2505.10543.pdf\n",
      "file_path: data\\2505.10543.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 328049\n",
      "creation_date: 2025-05-21\n",
      "last_modified_date: 2025-05-21\n",
      "\n",
      "arXiv:2505.10543v1  [cs.AI]  15 May 2025\n",
      "Towards a Deeper Understanding of Reasoning\n",
      "Capabilities in Large Language Models\n",
      "Annie Wong ,*, Thomas Bäck, Aske Plaat, Niki van Stein and Anna V . Kononova\n",
      "Leiden Institute of Advanced Computer Science\n",
      "Abstract. While large language models demonstrate impressive\n",
      "performance on static benchmarks, the true potential of large lan-\n",
      "guage models as self-learning and reasoning agents in dynamic envi-\n",
      "ronments remains unclear. This study systematically evaluates the ef-\n",
      "ficacy of self-reflection, heuristic mutation, and planning as prompt-\n",
      "ing techniques to test the adaptive capabilities of agents. We con-\n",
      "duct experiments with various open-source language models in dy-\n",
      "namic environments and find that larger models generally outperform\n",
      "smaller ones, but that strategic prompting can close this performance\n",
      "gap. Second, a too-long prompt can negatively impact smaller mod-\n",
      "els on basic reactive tasks, while larger models show more robust\n",
      "behaviour. Third, advanced prompting techniques primarily benefit\n",
      "smaller models on complex games, but offer less improvement for\n",
      "already high-performing large language models. Yet, we find that\n",
      "advanced reasoning methods yield highly variable outcomes: while\n",
      "capable of significantly improving performance when reasoning and\n",
      "decision-making align, they also introduce instability and can lead\n",
      "to big performance drops. Compared to human performance, our\n",
      "findings reveal little evidence of true emergent reasoning. Instead,\n",
      "large language model performance exhibits persistent limitations in\n",
      "crucial areas such as planning, reasoning, and spatial coordination,\n",
      "suggesting that current-generation large language models still suffer\n",
      "fundamental shortcomings that may not be fully overcome through\n",
      "self-reflective prompting alone. Reasoning is a multi-faceted task,\n",
      "and while reasoning methods like Chain of thought improves multi-\n",
      "step reasoning on math word problems, our findings using dynamic\n",
      "benchmarks highlight important shortcomings in general reasoning\n",
      "capabilities, indicating a need to move beyond static benchmarks to\n",
      "capture the complexity of reasoning.\n",
      "1 Introduction\n",
      "A key objective of artificial intelligence has been the development\n",
      "of intelligent agents that can perceive the environment and make au-\n",
      "tonomous decisions [31]. The emergence of large language models\n",
      "(LLMs) has significantly advanced this objective, showing strong ca-\n",
      "pabilities in the solution of various natural language processing tasks,\n",
      "such as solving mathematical problems, coding, reading comprehen-\n",
      "sion, translation, summarizing and answering questions [1, 4, 20].\n",
      "These findings show a promising direction towards autonomous\n",
      "agents, and recent studies have begun to explore methods for en-\n",
      "abling agents to learn dynamically. For instance, investigations into\n",
      "self-reflection mechanisms allow agents to evaluate their past actions\n",
      "and refine future strategies [22], while work on iterative prompting\n",
      "∗ Corresponding author. Email: a.s.w.wong@liacs.leidenuniv.nl\n",
      "and environmental feedback aims to facilitate continuous learning\n",
      "from experience [35]. LLMs have also been investigated to serve as\n",
      "a controller to solve environmental tasks through reasoning and plan-\n",
      "ning [33, 36, 19, 13]. The ability of LLM agents to learn and adapt in\n",
      "dynamic environments has yet to be definitively proven. While these\n",
      "models excel at in-context learning—generalizing from minimal ex-\n",
      "amples—their reliance on statistical prediction and lack of long-term\n",
      "memory often limit their effectiveness in dynamic settings [25]. Fur-\n",
      "thermore, achieving optimal performance on specialised tasks typi-\n",
      "cally requires either fine-tuning with human-annotated data [16] or\n",
      "reliance on careful prompt engineering [21]. These processes are\n",
      "resource-intensive, reducing the flexibility of deploying LLMs in\n",
      "real-world, constantly evolving applications. This study aims to un-\n",
      "derstand the following question: \" To what extent can LLM agents\n",
      "autonomously learn and adapt to novels tasks in dynamic environ-\n",
      "ments?\" Specifically, we investigate whether in-context mechanisms\n",
      "can improve continuous learning and multi-step reasoning across var-\n",
      "ious challenge levels. Our contributions are as follows 1:\n",
      "• We present a systematic comparison of open-souce LLMs on\n",
      "dynamic decision-making tasks. We quantify three prompting\n",
      "strategies—reflection, heuristic mutation , and planning—across\n",
      "SMART PLAY [32]: a benchmark to evaluate capabilities of intel-\n",
      "ligent agents. For simple reactive task, we find that excessive rea-\n",
      "soning harm performance for smaller models, while larger models\n",
      "are more robust. Larger models generally achieve higher scores in\n",
      "line with scaling laws [7], but carefully designed prompts can let\n",
      "small models match or surpass the baseline performance of larger\n",
      "models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "files = ['https://arxiv.org/pdf/2505.10543']\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "for idx, f in enumerate(files):\n",
    "    file_name = f\"./data/{f.split(\"/\")[-1]}.pdf\"\n",
    "    if not os.path.exists(file_name):\n",
    "        r = httpx.get(f, timeout=20)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./data/2505.10543.pdf\"]).load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Number of nodes after splitting: {len(nodes)}\")\n",
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a9eb0",
   "metadata": {},
   "source": [
    "### Configure LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a6e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"o4-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ee522",
   "metadata": {},
   "source": [
    "### Create indices\n",
    "\n",
    "Vector and Summary indices are created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fcd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828ccbe",
   "metadata": {},
   "source": [
    "### Create Query Engines.\n",
    "\n",
    "Combines doing lookups on the indices and query the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9879ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865409a1",
   "metadata": {},
   "source": [
    "Lets get a summary of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3cd22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper systematically evaluates self-reflection, heuristic mutation, and planning prompts on open-source LLMs in dynamic decision-making\n",
      "tasks, showing that while strategic prompting can boost smaller models, it yields unstable gains and highlights persistent reasoning,\n",
      "planning, and spatial coordination limitations.\n"
     ]
    }
   ],
   "source": [
    "response = summary_query_engine.query(\"Please provide a concise sentence that summarizes the document.\")\n",
    "wrapped_text = textwrap.fill(str(response), width=140, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37aed14",
   "metadata": {},
   "source": [
    "### Create The Tools \n",
    "\n",
    "Create the tools that provide metadata about the query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207c8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef89407",
   "metadata": {},
   "source": [
    "### Create A Router\n",
    "\n",
    "Below, a router query engine and its selector are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e9664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab265a",
   "metadata": {},
   "source": [
    "### Submitting Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d285124",
   "metadata": {},
   "source": [
    "#### Request A Summary Of The Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "613559eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document and its conclusions, which aligns with choice 1's focus on summarization..\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Provide a summary of the document and its conclusions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b9652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper investigates how well open-source large language models (LLMs) can learn, reason, and plan on the fly when faced with simple dynamic tasks,\n",
      "without any additional fine-tuning.  The authors assemble a unified “agent” framework that, at each time step, can be equipped with one or more of\n",
      "three in-context modules:  \n",
      "• Self-Reflection, which reviews the sequence of past states, actions and rewards to suggest how to improve future\n",
      "choices;  \n",
      "• Oracle (heuristic mutation), which evolves rule-like heuristics across episodes via a simple evolutionary strategy;  \n",
      "• Planner, which\n",
      "looks ahead a few steps by simulating possible action sequences and scoring their expected cumulative rewards.  \n",
      "\n",
      "They evaluate four models of\n",
      "increasing size (8 B to 70 B parameters) on four “SmartPlay” environments:  \n",
      "1. Two-armed bandit (exploration/exploitation)  \n",
      "2. Rock-Paper-Scissors\n",
      "(adapting to an opponent’s biased play)  \n",
      "3. Tower of Hanoi with three disks (spatial planning)  \n",
      "4. Messenger (navigating text descriptions, avoiding\n",
      "an enemy, delivering a message)  \n",
      "\n",
      "Key findings:  \n",
      "– Model size remains the single strongest predictor of raw performance; the largest model\n",
      "consistently beats smaller ones across all tasks.  \n",
      "– Carefully designed prompting (reflection, heuristics, planning) can sometimes let a mid-size\n",
      "model match or even surpass the baseline performance of a larger model—especially on complex tasks—but these gains are highly inconsistent.  In many\n",
      "runs the same prompt design causes a mid-sized model to do much worse than its own base prompt.  \n",
      "– On very simple reactive tasks (like the bandit),\n",
      "richer prompts actually degrade performance for smaller models: the extra context dilutes signal and leads them to “overthink” and explore too long\n",
      "instead of exploiting.  \n",
      "– Advanced prompting offers the largest upside for smaller models on tasks requiring multi-step reasoning or planning, but\n",
      "provides only marginal benefit to already strong large models.  \n",
      "– Across all models and tasks, there is little sign of genuine emergent reasoning or\n",
      "self-learning.  Agents still make basic mistakes: placing larger disks on smaller ones in Hanoi, getting stuck in loops, mis­identifying objects in\n",
      "the text-based navigation, and failing to form stable long-term plans.  \n",
      "\n",
      "The authors also show that transforming sparse rewards into a task-aligned\n",
      "dense reward signal can yield more reliable learning improvements than prompt tinkering alone.  Overall, these results argue that static benchmarks\n",
      "(e.g. question–answer pairs, math word problems) mask fundamental gaps in planning, spatial coordination, and adaptive reasoning, and they call for\n",
      "richer dynamic evaluations and for future work to combine in-context methods with external memory, symbolic reasoning layers, or grounded multimodal\n",
      "perception.\n"
     ]
    }
   ],
   "source": [
    "wrapped_text = textwrap.fill(str(response), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a45d83",
   "metadata": {},
   "source": [
    "Lets look at some of the metadata for the results. As expected, the router selected the summary query engine to service the query and, therefore, all 20 nodes of content were used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee0d3e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question asks for a summary of the document and its conclusions, which aligns with choice 1's focus on summarization.\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(response.metadata['selector_result'].reason)\n",
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb77cd",
   "metadata": {},
   "source": [
    "#### Ask A More Specific Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4544f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question seeks specific experimental results from the paper, so retrieving context is most relevant..\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the results of the Two-armed bandit evaluation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb5fdd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the two-armed bandit experiments the key findings were:\n",
      "\n",
      "• Baseline (simple count-and-exploit) wins for smaller and mid-sized models.  \n",
      "  – LLAMA\n",
      "3-8B: Baseline median ≈ 40.35 (CI 37.45–41.65) → Reflection+Planner drops to ≈ 34.00 (30.00–35.00).  \n",
      "  – DEEPSEEK-R1-14B: Baseline ≈ 41.00\n",
      "(40.55–41.40) → Reflection+Planner ≈ 32.05 (29.00–33.00).\n",
      "\n",
      "• Only the largest model benefits from more complex prompting.  \n",
      "  – LLAMA 3.3-70B:\n",
      "Baseline max ≈ 41.90 → Reflection+Planner max ≈ 48.00.\n",
      "\n",
      "• Why complexity hurts smaller models:  \n",
      "  1. Extra prompt text dilutes the reward‐count\n",
      "signal, lowering signal-to-noise.  \n",
      "  2. Reflection/Oracle/Planner encourage continued exploration even when one arm is clearly better, causing the\n",
      "agent to “overthink” and converge more slowly.\n",
      "\n",
      "• Overall, sheer model size drives the strongest performance; in-context prompting alone cannot fully\n",
      "bridge the gap.\n"
     ]
    }
   ],
   "source": [
    "wrapped_text = textwrap.fill(str(response), width=150, replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dee0e5",
   "metadata": {},
   "source": [
    "As this was a specific question, the specific context query engine was used with only 2 nodes used to generate the LLM response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question seeks specific experimental results from the paper, so retrieving context is most relevant.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(response.metadata['selector_result'].reason)\n",
    "print(len(response.source_nodes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
