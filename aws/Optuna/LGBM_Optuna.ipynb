{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output bucket: s3://rickmartelflood/model\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.sklearn import SKLearn\n",
    "\n",
    "instance_type = 'ml.m5.12xlarge' #ml.g4dn.xlarge(T4) #ml.m5.4xlarge(16), ml.m5.12xlarge (48) ml.m5.24xlarge (96)\n",
    "project_bucket = 'rickmartelflood'\n",
    "model_folder = 'model'\n",
    "model_output = 's3://{}/{}'.format(project_bucket, model_folder)\n",
    "print(f'Model output bucket: {model_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2024-05-26-22-11-50-407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:11:50 Starting - Starting the training job...\n",
      "2024-05-26 22:12:05 Starting - Preparing the instances for training...\n",
      "2024-05-26 22:12:43 Downloading - Downloading the training image...\n",
      "2024-05-26 22:13:08 Training - Training image download completed. Training in progress.....2024-05-26 22:13:47,613 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\n",
      "2024-05-26 22:13:47,615 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2024-05-26 22:13:47,618 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-26 22:13:47,632 sagemaker_sklearn_container.training INFO     Invoking user training script.\n",
      "2024-05-26 22:13:47,882 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/miniconda3/bin/python -m pip install -r requirements.txt\n",
      "Collecting matplotlib (from -r requirements.txt (line 1))\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting tqdm (from -r requirements.txt (line 2))\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 8.0 MB/s eta 0:00:00\n",
      "Collecting lightgbm (from -r requirements.txt (line 3))\n",
      "  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Collecting optuna (from -r requirements.txt (line 4))\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading fonttools-4.52.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.9/160.9 kB 31.8 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /miniconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.24.1)\n",
      "Collecting packaging>=20.0 (from matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /miniconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (10.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /miniconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 1)) (2.8.1)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: scipy in /miniconda3/lib/python3.8/site-packages (from lightgbm->-r requirements.txt (line 3)) (1.8.0)\n",
      "Collecting alembic>=1.5.0 (from optuna->-r requirements.txt (line 4))\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna->-r requirements.txt (line 4))\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna->-r requirements.txt (line 4))\n",
      "  Downloading SQLAlchemy-2.0.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting PyYAML (from optuna->-r requirements.txt (line 4))\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna->-r requirements.txt (line 4))\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4 (from alembic>=1.5.0->optuna->-r requirements.txt (line 4))\n",
      "  Downloading typing_extensions-4.12.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting importlib-metadata (from alembic>=1.5.0->optuna->-r requirements.txt (line 4))\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=3.1.0 (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 1))\n",
      "  Downloading zipp-3.19.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /miniconda3/lib/python3.8/site-packages (from sqlalchemy>=1.3.0->optuna->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /miniconda3/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna->-r requirements.txt (line 4)) (2.1.1)\n",
      "Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/9.2 MB 135.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 15.8 MB/s eta 0:00:00\n",
      "Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 119.1 MB/s eta 0:00:00\n",
      "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 380.1/380.1 kB 53.9 MB/s eta 0:00:00\n",
      "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.4/233.4 kB 42.3 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.1/301.1 kB 52.8 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.52.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 120.6 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 89.8 MB/s eta 0:00:00\n",
      "Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 11.9 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.2/103.2 kB 21.8 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 48.8 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 736.6/736.6 kB 75.5 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading zipp-3.19.0-py3-none-any.whl (8.3 kB)\n",
      "Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 16.5 MB/s eta 0:00:00\n",
      "Installing collected packages: zipp, typing-extensions, tqdm, PyYAML, pyparsing, packaging, Mako, kiwisolver, fonttools, cycler, contourpy, colorlog, sqlalchemy, lightgbm, importlib-resources, importlib-metadata, matplotlib, alembic, optuna\n",
      "Successfully installed Mako-1.3.5 PyYAML-6.0.1 alembic-1.13.1 colorlog-6.8.2 contourpy-1.1.1 cycler-0.12.1 fonttools-4.52.1 importlib-metadata-7.1.0 importlib-resources-6.4.0 kiwisolver-1.4.5 lightgbm-4.3.0 matplotlib-3.7.5 optuna-3.6.1 packaging-24.0 pyparsing-3.1.2 sqlalchemy-2.0.30 tqdm-4.66.4 typing-extensions-4.12.0 zipp-3.19.0\n",
      "2024-05-26 22:13:55,049 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2024-05-26 22:13:55,053 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-26 22:13:55,071 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2024-05-26 22:13:55,074 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-26 22:13:55,093 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2024-05-26 22:13:55,096 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-26 22:13:55,110 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"n_model_jobs\": 9,\n",
      "        \"n_study_jobs\": 5,\n",
      "        \"n_trials\": 20,\n",
      "        \"training_fraction\": 1.0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2024-05-26-22-11-50-407\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://rickmartelflood/model/sagemaker-scikit-learn-2024-05-26-22-11-50-407/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"hpo_ex1_case1\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"hpo_ex1_case1.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"n_model_jobs\":9,\"n_study_jobs\":5,\"n_trials\":20,\"training_fraction\":1.0}\n",
      "SM_USER_ENTRY_POINT=hpo_ex1_case1.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.m5.12xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.12xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=hpo_ex1_case1\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=48\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://rickmartelflood/model/sagemaker-scikit-learn-2024-05-26-22-11-50-407/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"n_model_jobs\":9,\"n_study_jobs\":5,\"n_trials\":20,\"training_fraction\":1.0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"sagemaker-scikit-learn-2024-05-26-22-11-50-407\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://rickmartelflood/model/sagemaker-scikit-learn-2024-05-26-22-11-50-407/source/sourcedir.tar.gz\",\"module_name\":\"hpo_ex1_case1\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"hpo_ex1_case1.py\"}\n",
      "SM_USER_ARGS=[\"--n_model_jobs\",\"9\",\"--n_study_jobs\",\"5\",\"--n_trials\",\"20\",\"--training_fraction\",\"1.0\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_HP_N_MODEL_JOBS=9\n",
      "SM_HP_N_STUDY_JOBS=5\n",
      "SM_HP_N_TRIALS=20\n",
      "SM_HP_TRAINING_FRACTION=1.0\n",
      "PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "/miniconda3/bin/python hpo_ex1_case1.py --n_model_jobs 9 --n_study_jobs 5 --n_trials 20 --training_fraction 1.0\n",
      "2024-05-26 22:13:55,111 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2024-05-26 22:13:55,111 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Arguments: Namespace(model_dir='/opt/ml/model', n_model_jobs=9, n_study_jobs=5, n_trials=20, output_dir='/opt/ml/output', training_fraction=1.0)\n",
      "/opt/ml/code\n",
      "Python 3.8.13\n",
      "[I 2024-05-26 22:14:14,851] A new study created in memory with name: no-name-99de817f-e9b9-4b8d-9c74-03824631a875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[I 2024-05-26 22:15:38,183] Trial 0 finished with value: 0.8606852805384444 and parameters: {'subsample': 0.19283916219826414, 'subsample_freq': 9, 'colsample_bytree': 0.2601231239306359, 'learning_rate': 0.003928867144346592, 'max_depth': 15, 'min_child_samples': 190, 'n_estimators': 850, 'num_leaves': 35, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 0 with value: 0.8606852805384444.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:16:14,676] Trial 3 pruned. \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[I 2024-05-26 22:17:02,571] Trial 1 finished with value: 0.8698570953928808 and parameters: {'subsample': 0.24344988058912453, 'subsample_freq': 6, 'colsample_bytree': 0.24482829185838043, 'learning_rate': 0.009550798346175041, 'max_depth': 75, 'min_child_samples': 10, 'n_estimators': 1300, 'num_leaves': 93, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 1 with value: 0.8698570953928808.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[I 2024-05-26 22:18:49,056] Trial 5 pruned. \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[I 2024-05-26 22:18:55,609] Trial 2 finished with value: 0.8705192413813979 and parameters: {'subsample': 0.5007756701864279, 'subsample_freq': 4, 'colsample_bytree': 0.37198745333564354, 'learning_rate': 0.006189068030302309, 'max_depth': 95, 'min_child_samples': 130, 'n_estimators': 1750, 'num_leaves': 74, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 2 with value: 0.8705192413813979.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:20:48,612] Trial 8 pruned. \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[I 2024-05-26 22:21:13,308] Trial 6 finished with value: 0.8708553651645765 and parameters: {'subsample': 0.2584934371591682, 'subsample_freq': 10, 'colsample_bytree': 0.518365639707148, 'learning_rate': 0.007055952015571047, 'max_depth': 20, 'min_child_samples': 40, 'n_estimators': 2350, 'num_leaves': 76, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 6 with value: 0.8708553651645765.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:22:31,044] Trial 10 pruned. \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[I 2024-05-26 22:22:31,711] Trial 7 finished with value: 0.871160710785776 and parameters: {'subsample': 0.9321441882060605, 'subsample_freq': 6, 'colsample_bytree': 0.837979598014033, 'learning_rate': 0.007326357964510739, 'max_depth': 100, 'min_child_samples': 125, 'n_estimators': 1200, 'num_leaves': 90, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 7 with value: 0.871160710785776.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[I 2024-05-26 22:22:55,579] Trial 11 pruned. \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:23:51,622] Trial 4 finished with value: 0.8709969345357896 and parameters: {'subsample': 0.9020696894368586, 'subsample_freq': 6, 'colsample_bytree': 0.5715512176160795, 'learning_rate': 0.0032467139158976676, 'max_depth': 60, 'min_child_samples': 90, 'n_estimators': 2450, 'num_leaves': 83, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 7 with value: 0.871160710785776.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:25:01,484] Trial 14 pruned. \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[I 2024-05-26 22:25:28,153] Trial 12 pruned. \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:26:40,781] Trial 15 pruned. \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:28:08,978] Trial 9 finished with value: 0.8710998273061282 and parameters: {'subsample': 0.8304693511626129, 'subsample_freq': 3, 'colsample_bytree': 0.5993820774006297, 'learning_rate': 0.0061266227818292576, 'max_depth': 50, 'min_child_samples': 70, 'n_estimators': 2750, 'num_leaves': 93, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 7 with value: 0.871160710785776.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1970\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[I 2024-05-26 22:28:54,441] Trial 13 pruned. \n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934365, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504271\n",
      "[I 2024-05-26 22:29:07,900] Trial 16 finished with value: 0.8710659637100499 and parameters: {'subsample': 0.7541893603732653, 'subsample_freq': 7, 'colsample_bytree': 0.8585028057756822, 'learning_rate': 0.00994061214998874, 'max_depth': 80, 'min_child_samples': 105, 'n_estimators': 1200, 'num_leaves': 55, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 7 with value: 0.871160710785776.\n",
      "[I 2024-05-26 22:29:16,158] Trial 17 pruned. \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504312\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1958\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504285\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 934366, number of used features: 36\n",
      "[LightGBM] [Info] Start training from score 0.504274\n",
      "[I 2024-05-26 22:30:34,596] Trial 18 finished with value: 0.8710914172356119 and parameters: {'subsample': 0.778495573667598, 'subsample_freq': 6, 'colsample_bytree': 0.7923891592637937, 'learning_rate': 0.009957699874588279, 'max_depth': 45, 'min_child_samples': 130, 'n_estimators': 1450, 'num_leaves': 59, 'metrics': 'r2', 'objective': 'regression'}. Best is trial 7 with value: 0.871160710785776.\n",
      "[I 2024-05-26 22:30:40,071] Trial 19 pruned. \n",
      "Study statistics: \n",
      "  Number of finished trials:  20\n",
      "  Number of pruned trials:  11\n",
      "  Number of complete trials:  9\n",
      "Best trial:\n",
      "  Value:  0.871160710785776\n",
      "  Params: \n",
      "    subsample: 0.9321441882060605\n",
      "    subsample_freq: 6\n",
      "    colsample_bytree: 0.837979598014033\n",
      "    learning_rate: 0.007326357964510739\n",
      "    max_depth: 100\n",
      "    min_child_samples: 125\n",
      "    n_estimators: 1200\n",
      "    num_leaves: 90\n",
      "    metrics: r2\n",
      "    objective: regression\n",
      "\n",
      "2024-05-26 22:31:37 Uploading - Uploading generated training model2024-05-26 22:31:36,126 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\n",
      "2024-05-26 22:31:49 Completed - Training job completed\n",
      "Training seconds: 1157\n",
      "Billable seconds: 484\n",
      "Managed Spot Training savings: 58.2%\n",
      "CPU times: user 2.66 s, sys: 100 ms, total: 2.76 s\n",
      "Wall time: 20min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aws_role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "image_uri = image_uris.retrieve(framework='sklearn', region='us-east-1',\n",
    "                    version='1.2-1', py_version='py3',\n",
    "                    image_scope='training',\n",
    "                    instance_type=instance_type)\n",
    "\n",
    "env = {'SAGEMAKER_REQUIREMENTS': 'requirements.txt'}\n",
    "\n",
    "model = SKLearn(\n",
    "    role=aws_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    output_path=model_output,\n",
    "    code_location=model_output,\n",
    "    entry_point=\"hpo_ex1_case1.py\",\n",
    "    source_dir='./container_scripts',\n",
    "    env=env,\n",
    "    image_uri=image_uri,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    hyperparameters={'n_trials': 20, \n",
    "                     'n_study_jobs': 5, 'n_model_jobs': 9,\n",
    "                     'training_fraction': 1.0},\n",
    "    use_spot_instances=True,\n",
    "    max_run=600*15, \n",
    "    max_wait=600*15,\n",
    ")\n",
    "\n",
    "model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
